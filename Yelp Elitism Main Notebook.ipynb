{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Instructions for feature development\n",
    "* [Load up dependencies](#Load-dependencies)\n",
    "* [Load the matrices and vectors from temp file](#Load-the-matrices-and-vectors)\n",
    "* [Add your own feature(s)](#Featurization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To-do:\n",
    "* Better+more feature engineering (check)\n",
    "* Try different ML models (check)\n",
    "* Clean up code, function definitions, naming\n",
    "* ONLY AT VERY END: run model on `test_set` and `test_set_labels`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import nltk, re\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Parsing & Structuring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "udata = []\n",
    "\n",
    "with open('yelp_academic_dataset_user.json') as user_data:    \n",
    "    for line in user_data:\n",
    "        udata.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'average_stars': 4.14,\n",
       " 'compliments': {'cool': 78,\n",
       "  'cute': 15,\n",
       "  'funny': 11,\n",
       "  'hot': 48,\n",
       "  'more': 3,\n",
       "  'note': 20,\n",
       "  'photos': 15,\n",
       "  'plain': 25,\n",
       "  'profile': 8,\n",
       "  'writer': 9},\n",
       " 'elite': [2005, 2006],\n",
       " 'fans': 69,\n",
       " 'friends': [],\n",
       " 'name': 'Russel',\n",
       " 'review_count': 108,\n",
       " 'type': 'user',\n",
       " 'user_id': '18kPq7GPye-YQ3LyKyAZPw',\n",
       " 'votes': {'cool': 245, 'funny': 166, 'useful': 278},\n",
       " 'yelping_since': '2004-10'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove friends so display isn't terrible (this user has a lot of friends)\n",
    "udata[0]['friends'] = []\n",
    "udata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdata = []\n",
    "with open('yelp_academic_dataset_review.json') as review_data:\n",
    "    for line in review_data:\n",
    "        rdata.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'business_id': 'vcNAWiLM4dR7D2nwwJ7nCA',\n",
       " 'date': '2007-05-17',\n",
       " 'review_id': '15SdjuK7DmYqUAj6rjGowg',\n",
       " 'stars': 5,\n",
       " 'text': \"dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\",\n",
       " 'type': 'review',\n",
       " 'user_id': 'Xqd0DzHaiyRqVH3WRG7hzg',\n",
       " 'votes': {'cool': 1, 'funny': 0, 'useful': 2}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bdata = []\n",
    "with open('yelp_academic_dataset_business.json') as business_data:\n",
    "    for line in business_data:\n",
    "        bdata.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attributes': {'By Appointment Only': True},\n",
       " 'business_id': 'vcNAWiLM4dR7D2nwwJ7nCA',\n",
       " 'categories': ['Doctors', 'Health & Medical'],\n",
       " 'city': 'Phoenix',\n",
       " 'full_address': '4840 E Indian School Rd\\nSte 101\\nPhoenix, AZ 85018',\n",
       " 'hours': {'Friday': {'close': '17:00', 'open': '08:00'},\n",
       "  'Monday': {'close': '17:00', 'open': '08:00'},\n",
       "  'Thursday': {'close': '17:00', 'open': '08:00'},\n",
       "  'Tuesday': {'close': '17:00', 'open': '08:00'},\n",
       "  'Wednesday': {'close': '17:00', 'open': '08:00'}},\n",
       " 'latitude': 33.499313,\n",
       " 'longitude': -111.983758,\n",
       " 'name': 'Eric Goldberg, MD',\n",
       " 'neighborhoods': [],\n",
       " 'open': True,\n",
       " 'review_count': 9,\n",
       " 'stars': 3.5,\n",
       " 'state': 'AZ',\n",
       " 'type': 'business'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def businesses_set(bdata):\n",
    "    biz_set = set([])\n",
    "    US_states = ['AZ', 'NV', 'WI', 'IL', 'NC', 'PA']\n",
    "    \n",
    "    for biz in bdata:\n",
    "        if biz['state'] in US_states:\n",
    "            biz_set.add(biz['business_id'])\n",
    "    return biz_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biz_set = businesses_set(bdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reviews_hash(rdata, biz_set):\n",
    "    reviews_dict = {}\n",
    "    \n",
    "    for review in rdata:\n",
    "        if review['business_id'] in biz_set:\n",
    "            user_id = review['user_id']\n",
    "            year = int(review['date'][:4])\n",
    "            if year != 2015:\n",
    "                if (user_id, year) in reviews_dict:\n",
    "                    reviews_dict[(user_id, year)].append(review)\n",
    "                else:\n",
    "                    reviews_dict[(user_id, year)] = [review]\n",
    "    return reviews_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews_dict = reviews_hash(rdata, biz_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def elite_review_years_hash(udata):\n",
    "    elite_review_years_dict = {}\n",
    "    \n",
    "    udata_subset = [user for user in udata if user['review_count']>=20]\n",
    "    for user in udata_subset:\n",
    "        user_id = user['user_id']\n",
    "        elite_years = user['elite']\n",
    "        elite_review_years_dict[user_id] = [year-1 for year in elite_years]\n",
    "    return elite_review_years_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elite_review_years_dict = elite_review_years_hash(udata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# returns 2 arrays\n",
    "# 1) each entry is array of all of a given user's reviews in a given year\n",
    "# 2) each entry is corresponding elite (1) or non-elite (0) status in next year\n",
    "# i.e. whether reviews are \"elite-worthy\"\n",
    "def final_raw_data_and_vector(reviews_dict, elite_review_years_dict):\n",
    "    user_years_in_review = []\n",
    "    elite_labels = []\n",
    "    \n",
    "    test_user_years_in_review = []\n",
    "    test_elite_labels = []\n",
    "\n",
    "    for id_year_tuple, reviews in reviews_dict.items():\n",
    "        user_id = id_year_tuple[0]\n",
    "        year = id_year_tuple[1]\n",
    "        \n",
    "        if year == 2014:\n",
    "            if user_id in elite_review_years_dict.keys():        \n",
    "                test_user_years_in_review.append(reviews)\n",
    "\n",
    "                if year in elite_review_years_dict[user_id]:\n",
    "                    test_elite_labels.append(1)\n",
    "                else:\n",
    "                    test_elite_labels.append(0)                \n",
    "        else:\n",
    "            if user_id in elite_review_years_dict.keys():        \n",
    "                user_years_in_review.append(reviews)\n",
    "\n",
    "                if year in elite_review_years_dict[user_id]:\n",
    "                    elite_labels.append(1)\n",
    "                else:\n",
    "                    elite_labels.append(0)\n",
    "    return user_years_in_review, elite_labels, test_user_years_in_review, test_elite_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.76 s\n"
     ]
    }
   ],
   "source": [
    "% time train_dev_set, train_dev_labels, test_set, test_labels = final_raw_data_and_vector(reviews_dict, elite_review_years_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119469\n"
     ]
    }
   ],
   "source": [
    "assert len(train_dev_set) == len(train_dev_labels)\n",
    "train_dev_set_len = len(train_dev_labels)\n",
    "print(train_dev_set_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34770\n"
     ]
    }
   ],
   "source": [
    "assert len(test_set) == len(test_labels)\n",
    "test_set_len = len(test_labels)\n",
    "print(test_set_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of train/dev set to test set: 0.7745706338863712\n"
     ]
    }
   ],
   "source": [
    "total_len = train_dev_set_len + test_set_len\n",
    "print(\"Proportion of train/dev set to test set: \"+str(train_dev_set_len/total_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_train_dev_set = np.array(train_dev_set)\n",
    "np_train_dev_labels = np.array(train_dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rand_indices = np.random.permutation(len(train_dev_labels))\n",
    "shuffled_train_dev_set = np_train_dev_set[rand_indices]\n",
    "shuffled_train_dev_labels = np_train_dev_labels[rand_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset_length = len(shuffled_train_dev_set)\n",
    "# 80-20 between training, and dev (within training+dev)\n",
    "train_set = shuffled_train_dev_set[:subset_length*4//5]\n",
    "train_set_labels = shuffled_train_dev_labels[:subset_length*4//5]\n",
    "\n",
    "dev_set = shuffled_train_dev_set[subset_length*4//5:]\n",
    "dev_set_labels = shuffled_train_dev_labels[subset_length*4//5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reviews_to_text(reviews_arr):\n",
    "    # Produce just the text of reviews\n",
    "    reviews_text = []\n",
    "    for review in reviews_arr:\n",
    "        reviews_text.append(review['text'])\n",
    "    return reviews_text\n",
    "\n",
    "def tokenize_text(reviews_text):\n",
    "    # Returns list of reviews => list of sentences => list of words\n",
    "    tokenized_reviews = []\n",
    "    for review in reviews_text:\n",
    "        raw_sents = sent_tokenizer.tokenize(review)\n",
    "        tokenized_sents = [nltk.word_tokenize(word) for word in raw_sents]\n",
    "        tokenized_reviews.append(tokenized_sents)\n",
    "    return tokenized_reviews\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    tokenized_dataset = []\n",
    "    \n",
    "    for reviews_arr in dataset:\n",
    "        tokenized_reviews = tokenize_text(reviews_to_text(reviews_arr))\n",
    "        tokenized_dataset.append(tokenized_reviews)\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_train_set = tokenize_dataset(train_set)\n",
    "tokenized_dev_set   = tokenize_dataset(dev_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Save this matrix and vector to a temp file, so info is loadable quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file = open('data_sets2.npz', 'wb+')\n",
    "np.savez(data_file, train_set, train_set_labels, dev_set, dev_set_labels, tokenized_train_set, tokenized_dev_set, test_set, test_labels)\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load the matrices and vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remember to import dependencies first\n",
    "load_data = np.load('data_sets.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arr_7', 'arr_1', 'arr_2', 'arr_4', 'arr_5', 'arr_0', 'arr_3', 'arr_6']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arrays in order are\n",
    "# train_set, train_set_labels, dev_set, dev_set_labels, tokenized_train_set, tokenized_dev_set, test_set, test_labels\n",
    "load_data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = load_data['arr_0']\n",
    "train_set_labels = load_data['arr_1']\n",
    "dev_set = load_data['arr_2']\n",
    "dev_set_labels = load_data['arr_3']\n",
    "tokenized_train_set = load_data['arr_4']\n",
    "tokenized_dev_set = load_data['arr_5']\n",
    "test_set = load_data['arr_6']\n",
    "test_labels = load_data['arr_7']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes before you begin:\n",
    "* Remember to step through each feature definition below\n",
    "* Then define your feature function, if necessary\n",
    "* Set your feature to some variable in the featurize() function\n",
    "* Make sure feature is appended in featurize(): `feature_entry.append(YOUR_FEATURE_VARIABLE)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def total_reviews(reviews_arr):\n",
    "    return len(reviews_arr)\n",
    "\n",
    "def basic_totals(reviews_arr):\n",
    "    basic_arr = []\n",
    "    \n",
    "    chars = 0\n",
    "    paragraphs = 0\n",
    "    cool_votes = 0\n",
    "    funny_votes = 0\n",
    "    useful_votes = 0\n",
    "    \n",
    "    for review in reviews_arr:\n",
    "        chars += len(review['text'])\n",
    "        paragraphs += review['text'].count(\"\\n\\n\")\n",
    "        cool_votes += review['votes']['cool']\n",
    "        funny_votes += review['votes']['funny']\n",
    "        useful_votes += review['votes']['useful']\n",
    "    \n",
    "    basic_arr.append(chars)\n",
    "    basic_arr.append(paragraphs)\n",
    "    basic_arr.append(cool_votes)\n",
    "    basic_arr.append(funny_votes)\n",
    "    basic_arr.append(useful_votes)\n",
    "    \n",
    "    return basic_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_NLP_features(tokenized_reviews):\n",
    "    NLP_features = []\n",
    "    \n",
    "    total_sents = 0\n",
    "    total_words = 0\n",
    "    vocabulary = set([])\n",
    "    \n",
    "    for review in tokenized_reviews:\n",
    "        for sent in review:\n",
    "            total_sents += 1\n",
    "            total_words += len(sent) - 1\n",
    "            for word in sent:\n",
    "                if word.lower() not in vocabulary:\n",
    "                    vocabulary.add(word.lower())\n",
    "\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    \n",
    "    return [total_sents, total_words, vocabulary_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def featurize(dataset, tokenized_dataset):\n",
    "    feature_array = []\n",
    "    index = 0\n",
    "    \n",
    "    for reviews_arr in dataset:\n",
    "        feature_entry = []\n",
    "\n",
    "        review_count = total_reviews(reviews_arr)\n",
    "        totals = basic_totals(reviews_arr)\n",
    "        \n",
    "        totals.extend(get_NLP_features(tokenized_dataset[index]))\n",
    "        \n",
    "        averages = [total/review_count for total in totals]\n",
    "\n",
    "        feature_entry.append(review_count)\n",
    "        feature_entry.extend(totals)\n",
    "        feature_entry.extend(averages)\n",
    "        \n",
    "        feature_array.append(feature_entry)\n",
    "        index += 1\n",
    "    return feature_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_dict = {\n",
    "    1: \"total reviews\",\n",
    "    2: \"total characters\",\n",
    "    3: \"total paragraphs\",\n",
    "    4: \"total cool votes\",\n",
    "    5: \"total funny votes\",\n",
    "    6: \"total useful votes\",\n",
    "    7: \"total sentences\",\n",
    "    8: \"total words\",\n",
    "    9: \"total size of vocabulary (unique words)\",\n",
    "    10: \"chars per review\",\n",
    "    11: \"paragraphs per review\",\n",
    "    12: \"cool votes per review\",\n",
    "    13: \"funny votes per review\",\n",
    "    14: \"useful votes per review\",\n",
    "    15: \"sentences per review\",\n",
    "    16: \"words per review\",\n",
    "    17: \"size of vocabulary per review\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "% time featurized_train = featurize(train_set, tokenized_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "% time featurized_dev = featurize(dev_set, tokenized_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_test_set = tokenize_dataset(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "% time featurized_test = featurize(test_set, tokenized_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 322 ms\n"
     ]
    }
   ],
   "source": [
    "% time normalized_train = normalize(featurized_train, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 71.6 ms\n"
     ]
    }
   ],
   "source": [
    "% time normalized_dev = normalize(featurized_dev, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 119 ms\n"
     ]
    }
   ],
   "source": [
    "% time normalized_test = normalize(featurized_test, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_index = np.where(train_set_labels == 1)[0]\n",
    "zero_index = np.where(train_set_labels == 0)[:len(one_index)][0]\n",
    "\n",
    "subset_train = normalized_train[list(one_index)+list(zero_index)]\n",
    "subset_labels = train_set_labels[list(one_index)+list(zero_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set percentage elite: 0.23506147004969918\n",
      "development set percentage elite: 0.23492236219813334\n"
     ]
    }
   ],
   "source": [
    "# Making sure the percentage of elite users is reasonable\n",
    "def get_elite_ratio(labels):\n",
    "    elite = 0\n",
    "    for item in labels:\n",
    "        if item == 1:\n",
    "            elite += 1\n",
    "    return elite / len(labels)\n",
    "print(\"training set percentage elite: \" +str(get_elite_ratio(train_set_labels)))\n",
    "print(\"development set percentage elite: \" +str(get_elite_ratio(dev_set_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_model_stats(dev_set, dev_set_labels, clf):\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    true_neg = 0\n",
    "    false_neg = 0\n",
    "    total = 0\n",
    "    \n",
    "    for element in dev_set:\n",
    "        correct_label = dev_set_labels[total]\n",
    "        \n",
    "        if clf.predict(element) == correct_label:\n",
    "            if correct_label == 0:\n",
    "                true_neg += 1\n",
    "            else:\n",
    "                true_pos += 1\n",
    "        else:\n",
    "            if correct_label == 0:\n",
    "                false_pos += 1\n",
    "            else:\n",
    "                false_neg += 1\n",
    "        total += 1\n",
    "        \n",
    "    accuracy = (true_pos + true_neg) / total\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    f1_score = 2 * ((precision * recall) / (precision + recall))\n",
    "    \n",
    "    print(\"T Positive: %s, F Positive: %s\" % (true_pos, false_pos))\n",
    "    print(\"F Negative: %s, T Negative: %s\" % (false_neg, true_neg))\n",
    "    print()\n",
    "    print(\"The following metrics are on a scale of 0 to 1:\")\n",
    "    print(\"Model accuracy: \"+str(accuracy))\n",
    "    print(\"Model precision: \"+str(precision))\n",
    "    print(\"Model recall: \"+str(recall))\n",
    "    print(\"Model F1 Score: \"+str(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Gaussian Naive Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a naive bayes model has quite low accuracy as well as precision. Its accuracy is lower than always guessing non-elite, and its precision of 29.47% means that out of the times it guesses elite, it is only correct that percentage of the time, which is definitely not great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Normalized Features Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 71.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_clf = GaussianNB()\n",
    "% time gaussian_clf.fit(normalized_train, train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T Positive: 2296, F Positive: 5496\n",
      "F Negative: 3317, T Negative: 12784\n",
      "\n",
      "The following metrics are on a scale of 0 to 1:\n",
      "Model accuracy: 0.6311471979240781\n",
      "Model precision: 0.2946611909650924\n",
      "Model recall: 0.40905041867094244\n",
      "Model F1 Score: 0.34255874673629244\n",
      "Wall time: 2.33 s\n"
     ]
    }
   ],
   "source": [
    "% time get_model_stats(normalized_dev, dev_set_labels, gaussian_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Non-normalized Features Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 393 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_clf = GaussianNB()\n",
    "% time gaussian_clf.fit(featurized_train, train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T Positive: 1001, F Positive: 761\n",
      "F Negative: 4612, T Negative: 17519\n",
      "\n",
      "The following metrics are on a scale of 0 to 1:\n",
      "Model accuracy: 0.7751224207927008\n",
      "Model precision: 0.5681044267877412\n",
      "Model recall: 0.17833600570105113\n",
      "Model F1 Score: 0.2714576271186441\n",
      "Wall time: 3.02 s\n"
     ]
    }
   ],
   "source": [
    "% time get_model_stats(featurized_dev, dev_set_labels, gaussian_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Non-normalized Features Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An SVM-based model was prohibitivly slow to run and not particularly accurate, so we decided against using an SVM Model. The lack of wall time for the learning machine fitting was a result of us changing the featurization a little bit, which caused the machine to take over 45 minutes to run, at which point we just stopped the kernel and decided not to proceed with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_clf = svm.SVC()\n",
    "% time svm_clf.fit(featurized_train, train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 76.34453605658561 percent.\n",
      "Wall time: 57.9 s\n"
     ]
    }
   ],
   "source": [
    "% time get_model_accuracy(featurized_dev, dev_set_labels, svm_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Subset Normalized Features Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 626 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_clf = linear_model.LogisticRegression(C=1)\n",
    "% time logreg_clf.fit(subset_train, subset_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a linear logistic regression model performs **very poorly**: it has an accuracy of 77.27 percent _when run on the data on which it was trained_. We know that the dev set has 0.2349 (23.49%) of its data labeled \"non-elite\", so by guessing \"non-elite\" every time, it will achieve 76.51% accuracy. As we see that it is a poor match when run on that data on which it was trained, we know then that this model must be lacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T Positive: 1206, F Positive: 456\n",
      "F Negative: 21260, T Negative: 72653\n",
      "\n",
      "The following metrics are on a scale of 0 to 1:\n",
      "Model accuracy: 0.7727857703374313\n",
      "Model precision: 0.7256317689530686\n",
      "Model recall: 0.05368111813406926\n",
      "Model F1 Score: 0.09996684350132626\n",
      "Wall time: 4.63 s\n"
     ]
    }
   ],
   "source": [
    "% time get_model_stats(subset_train, subset_labels, logreg_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T Positive: 396, F Positive: 270\n",
      "F Negative: 5217, T Negative: 18010\n",
      "\n",
      "The following metrics are on a scale of 0 to 1:\n",
      "Model accuracy: 0.7703511488720546\n",
      "Model precision: 0.5945945945945946\n",
      "Model recall: 0.07055050774986638\n",
      "Model F1 Score: 0.12613473483038698\n",
      "Wall time: 1.2 s\n"
     ]
    }
   ],
   "source": [
    "% time get_model_stats(normalized_dev, dev_set_labels, logreg_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Random Forests Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we found that a random forests model performed the best on this data. We did not achieve an accuracy that was wildly better than baseline, but given all of the metrics we used here, it seems that, on the whole, random forests performed well for this binary classification application. Running the classifier on features without normalization seemed to perform the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Normalized Features Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.48 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randfor_clf = RandomForestClassifier(n_estimators=40, max_depth=5)\n",
    "% time randfor_clf.fit(normalized_train, train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T Positive: 1618, F Positive: 1546\n",
      "F Negative: 3995, T Negative: 16734\n",
      "\n",
      "The following metrics are on a scale of 0 to 1:\n",
      "Model accuracy: 0.7680910726991169\n",
      "Model precision: 0.511378002528445\n",
      "Model recall: 0.28825939782647425\n",
      "Model F1 Score: 0.3686908966617295\n",
      "Wall time: 54.5 s\n"
     ]
    }
   ],
   "source": [
    "% time get_model_stats(normalized_dev, dev_set_labels, randfor_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Non-normalized Features Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.35 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randfor_clf = RandomForestClassifier(n_estimators=40, max_depth=5)\n",
    "% time randfor_clf.fit(featurized_train, train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T Positive: 842, F Positive: 344\n",
      "F Negative: 4771, T Negative: 17936\n",
      "\n",
      "The following metrics are on a scale of 0 to 1:\n",
      "Model accuracy: 0.7859205625078475\n",
      "Model precision: 0.7099494097807757\n",
      "Model recall: 0.15000890789239266\n",
      "Model F1 Score: 0.24768348286512723\n",
      "Wall time: 55.9 s\n"
     ]
    }
   ],
   "source": [
    "% time get_model_stats(featurized_dev, dev_set_labels, randfor_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuracy = correct predictions / total predictions\n",
    "# It is the number of correct predictions made divided by the total number of predictions made\n",
    "\n",
    "# precision = true positives / (true positives + false positives)\n",
    "# Precision can be thought of as a measure of a classifiers exactness.\n",
    "# A low precision can also indicate a large number of False Positives.\n",
    "\n",
    "# recall = true positives / (true positives + false negatives)\n",
    "# Recall can be thought of as a measure of a classifiers completeness.\n",
    "# A low recall indicates many False Negatives.\n",
    "# f1 = 2 * ((precision * recall) / (precision + recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Running Model on Test Set (Non-normalized Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T Positive: 1143, F Positive: 574\n",
      "F Negative: 4680, T Negative: 28373\n",
      "\n",
      "The following metrics are on a scale of 0 to 1:\n",
      "Model accuracy: 0.8488927236123095\n",
      "Model precision: 0.6656959813628421\n",
      "Model recall: 0.19629057187017002\n",
      "Model F1 Score: 0.303183023872679\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "% time get_model_stats(featurized_test, test_labels, randfor_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0033745   0.01645483  0.13675816  0.233239    0.12038536  0.07016558\n",
      "  0.00295394  0.03919613  0.03617639  0.0198593   0.09358085  0.12706954\n",
      "  0.03867808  0.01311949  0.00468971  0.02569395  0.0186052 ]\n"
     ]
    }
   ],
   "source": [
    "print(randfor_clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_vals = sorted(randfor_clf.feature_importances_, reverse=True)[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 features in order from most to least important:\n",
      "Rank: 1   |  Feature: total paragraphs       |   Importance score: 0.233239\n",
      "Rank: 2   |  Feature: total characters       |   Importance score: 0.136758\n",
      "Rank: 3   |  Feature: paragraphs per review  |   Importance score: 0.127070\n",
      "Rank: 4   |  Feature: total cool votes       |   Importance score: 0.120385\n",
      "Rank: 5   |  Feature: chars per review       |   Importance score: 0.093581\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 5 features in order from most to least important:\")\n",
    "index = 1\n",
    "for item in max_vals:\n",
    "    item_index = np.where(randfor_clf.feature_importances_== item)[0][0]\n",
    "    print(\"Rank: %-3d |  Feature: %-22s |   Importance score: %f\" % (index, feature_dict[item_index], item))\n",
    "    index += 1"
   ]
  }
 ],
 "metadata": {
  "gist_id": "c42ed46055d66349747a",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
